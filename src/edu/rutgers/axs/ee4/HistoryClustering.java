package edu.rutgers.axs.ee4;

import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.search.*;

import java.io.*;
import java.util.*;
import java.util.regex.*;
import java.text.*;

import javax.persistence.*;

import org.json.*;

//import cern.colt.matrix.*;
//import cern.colt.matrix.impl.SparseDoubleMatrix2D;
//import cern.colt.matrix.linalg.SingularValueDecomposition;

import edu.rutgers.axs.*;
import edu.rutgers.axs.indexer.*;
import edu.rutgers.axs.sql.*;
import edu.rutgers.axs.recommender.*;


/** Document clustering based on user-ArXiv.org interaction history, for
    Peter Frazier's Exploration Engine ver. 4. For details, see PF's
    2013-06-07 message, "new clustering scheme (was Re: EE4 developments)".
    
 */
public class HistoryClustering {

    /** An auxiliary class used when reading in and preprocessing the
	(user,page) matrix. For each user id, we store a vector of
	pages he's accessed.
     */
    static private class U2PL  {

	private HashMap<String, HashSet<Integer>> map = new HashMap<String, HashSet<Integer>>();

	/** This map is used during the table-building process only */
	private Vector<String> origno2aid = new Vector<String>();
	private HashMap<String, Integer> aid2origno = new HashMap<String, Integer>();
	
	/** The final numeric map for article IDs.	 */
	String[] no2aid;
	HashMap<String, Integer> aid2no;

	/** The numeric map for IP hash values (which are a surrogate
	    for user identifiers). 	 */
	String[] no2u;
	HashMap<String, Integer> u2no;

	private Integer registerPage(String p) {
	    Integer q = aid2origno.get(p);
	    if (q==null) {
		q = new Integer(origno2aid.size());
		origno2aid.add(p);
		aid2origno.put(p,q);
	    }
	    return q;
	}

	void add(String u, String p) {
	    HashSet<Integer> v = map.get(u);
	    if (v==null) map.put(u, v = new HashSet<Integer>());
	    v.add(registerPage(p));
	}


	/** Removes "low activity" users and papers; converts the rest
	    into a SparseDoubleMatrix2Dx object, for use with SVD.
	*/
	SparseDoubleMatrix2Dx toMatrix() {
	    final int user_thresh=2,  paper_thresh=2;

	    long origCnt = 0;
	    for(Iterator<Map.Entry<String,HashSet<Integer>>> it = map.entrySet().iterator();
		it.hasNext(); ) {
		origCnt += it.next().getValue().size();
	    }


	    System.out.println("Processing the view matrix. Originally, there are " + map.size() + " users, and " + origCnt + " nonzeros");


	    for(Iterator<Map.Entry<String,HashSet<Integer>>> it = map.entrySet().iterator();
		it.hasNext(); ) {
		if (it.next().getValue().size()< user_thresh) it.remove();
	    }

	    System.out.println("Only " + map.size() + " users have at least " + user_thresh + " page views");
	    
	    // view count for each page
	    int[] viewCnt = new int[origno2aid.size()];

	    for(HashSet<Integer> v: map.values()) {
		for(Integer origno: v) {
		    viewCnt[origno.intValue()] ++;
		}
	    }
	    
	    int viewedPagesCnt=0, popularPagesCnt=0;
	    int cap = 0;
	    for(int vc:  viewCnt) {
		if (vc>0)  viewedPagesCnt++;
		if (vc>=paper_thresh)  {
		    popularPagesCnt++;
		    cap += vc;
		}
	    }

	    System.out.println("There are " + viewCnt.length + " papers; among them, " + viewedPagesCnt + " have been viewed by at least 1 'relevant' user, and " +  popularPagesCnt  + " have been viewed by at least " + paper_thresh + " relevant users");

	    System.out.println("The view matrix will have " +cap+ " nonzeros");

	    // create the permanent page map (only including "popular" pages)
	    no2aid = new String[  popularPagesCnt ];
	    aid2no = new HashMap<String, Integer>();
	    int k = 0;
	    for(int i=0; i<viewCnt.length; i++) {		
		if (viewCnt[i]<paper_thresh)  continue;
		String aid = origno2aid.elementAt(i);
		aid2no.put(aid, new Integer(k));
		no2aid[k++] = aid;
	    }
	
	    no2u = new String[ map.size() ];
	    u2no = new HashMap<String, Integer>();
	    k = 0;
	    for(String u: map.keySet()) {
		u2no.put(u, new Integer(k));
		no2u[k++] = u;
	    }

	    SparseDoubleMatrix2Dx mat2 = new SparseDoubleMatrix2Dx(map.size(), no2aid.length);
	    for(String u: map.keySet()) {
		int row = u2no.get(u).intValue();
		int cnt = 0;
		for(Integer _origno: map.get(u)) {
		    int origno = _origno.intValue();
		    if (viewCnt[origno] >= paper_thresh) cnt++;
		}
		int pos[] = new int[cnt];

		k=0;
		for(Integer _origno: map.get(u)) {
		    int origno = _origno.intValue();
		    if (viewCnt[origno] < paper_thresh)  continue;
		    String aid = origno2aid.elementAt(origno);
		    int col = aid2no.get(aid).intValue();
		    //mat2.setQuick(row, col, 1.0);
		    pos[k++] = col;
		}
		mat2.setRowSameValue(row, pos, 1.0);
	    }
	    System.out.println("Have a " + mat2.rows() + " by " + mat2.columns() + " matrix with " + mat2.cardinality()  + " non-zeros");
	    return mat2;
	}
    }


    /** Reads split files for a particular category generated by DataSaver.
	All files found in the category's splt file directory will be read;
	so make sure you have the correct selection of files there.
     */
    static U2PL	readSplitFiles(String majorCat, ArxivUserInferrer inferrer) throws IOException {
	File dir = Json.catDir(majorCat);
	File[] files=dir.listFiles(); 
	U2PL   user2pageList = new U2PL();

	for(File f: files) {
	    System.out.println("Reading split file " + f);
	    FileReader fr = new FileReader(f);
	    LineNumberReader r = new LineNumberReader(fr);
	    String s = null;
	    while((s=r.readLine())!=null) {
		s = s.trim();
		if (s.equals("")) continue;
		String q[] = s.split(",");
		if (q.length!=3) throw new IOException("Could not parse line no. " + r.getLineNumber() + " in file " + f + " as an (ip,cookie,page) tuple:\n" + s);
		String user = inferrer.inferUser(q[0],q[1]);
		if (user==null) {
		} else {
		    user2pageList.add(user, q[2]);
		}
	    }
	    r.close();
	}
	System.out.println( inferrer.report());
	return user2pageList;
    }

    static void doSvd(String majorCat, 	ArxivUserInferrer inferrer)  throws IOException {
	SparseDoubleMatrix2Dx mat;
	String[] no2aid;
	{
	    U2PL user2pageList = readSplitFiles(majorCat, inferrer);
	    mat = user2pageList.toMatrix();
	    no2aid = user2pageList.no2aid;
	}

	//boolean useMySVD = false;
	boolean useMySVD = true;

	System.out.println("Doing SVD");
	final int k_svd = 5; // number of singular vectors to keep
	int keepSvd = k_svd;

	double[] sval;
	double[][] qq;

	if (useMySVD) {
	    // testing our own code
	    SVD svd = new SVD(mat);
	    svd.findTopSingularVectors(k_svd);
	    sval = svd.getSingularValues();
	    qq = svd.vIntoArrayOfRows();
	} else {
	    throw new AssertionError("COLT SVD no longer supported");
	    /*
	    SingularValueDecomposition svd=new SingularValueDecomposition(mat);

	    sval = svd.getSingularValues();
	    DoubleMatrix2D v = svd.getV();

	    // cluster *rows* of V
	    qq = intoArrayOfRows(v, keepSvd);
	    */
	}

	keepSvd = (k_svd < sval.length)? k_svd : sval.length;
	System.out.print("Top " +  keepSvd + " singular values:");
	for(int i=0; i< keepSvd ; i++) System.out.print(" " +sval[i]);
	System.out.println();

	Vector<DenseDataPoint> vdoc = new Vector<DenseDataPoint>(qq.length);
	for(double[] q: qq) {
	    vdoc.add(new DenseDataPoint(q));
	}

	// desired number of clusters
	int k_kmeans = (int)Math.sqrt(  (double)vdoc.size()/200.0);
	if (k_kmeans <=1) k_kmeans = 1;

	KMeansClustering clu = KMeansClustering.findBestClustering(vdoc,
								   keepSvd,
								   k_kmeans);

	int id0 = 1;
	saveAsg( clu, no2aid, majorCat, id0);
    }

    /** Converts a section (first keepSvd columns) of a DoubleMatrix2D
	into a 2D array of doubles.  The first keepSvd elements of each row
	will be packaged into an arrray of doubles. */
    /*
    static private double[][] intoArrayOfRows(SparseDoubleMatrix2D v, int keepSvd) {
	
	int ndoc  = v.rows();
	double[][] z = new double[ndoc][];

	for(int i=0; i<ndoc; i++) {
	    double[] q= new double[keepSvd];
	    for(int j=0; j<q.length; j++) {
		q[j] = 	v.getQuick(i, j); 
	    }
	    z[i] = q;
	}
	return z;
    }
    */
    static private File getAsgDirPath(String cat)  {
	File d = DataFile.getMainDatafileDirectory();
	d = new File(d,  "tmp");
	d = new File(d,  "svd-asg");
	d = new File(d, cat);
	return d;
    }


    private static void saveAsg(KMeansClustering clu, String[] no2aid, String cat, int id0) throws IOException {
	File catdir =  getAsgDirPath(cat);
	catdir.mkdirs();
	File f = new File(catdir, "asg.dat");
	PrintWriter w= new PrintWriter(new FileWriter(f));
	for(int i=0; i<no2aid.length; i++) {
	    int id = id0 + clu.asg[i];
	    String aid = no2aid[i];
	    w.println(aid + "," + id);
	}
	w.close();
    }

    static void usage() {
	usage(null);
    }

    static void usage(String m) {
	System.out.println("Usage: HistoryClustering [split filename|svd cat]");
	if (m!=null) {
	    System.out.println(m);
	}
	System.exit(1);
    }


    public static void main(String [] argv) throws IOException, JSONException {

	if (argv.length < 1) {
	    usage("Command not specified");
	} else if (argv[0].equals("split")) {
	    if (argv.length < 2) {
		usage("File name not specified");
	    }
	    // String fname = "../json/user_data_0/" + "100510_user_data.json";
	    //String fname = "../json/user_data/" + "110301_user_data.json";
	    String fname = argv[1];
	    Json.splitJsonFile(fname);
	} else if (argv[0].equals("svd")) {
	    if (argv.length < 2) {
		usage("Category name not specified");
	    }
	    String majorCat = argv[1];	
	    final boolean useCookies=true;

	    final String tcPath = "/data/json/usage/tc.json.gz";
	    ArxivUserInferrer inferrer = useCookies?
		new CookieArxivUserInferrer(new ArxivUserTable(tcPath)):
		new IPArxivUserInferrer();
	    doSvd(majorCat,inferrer);
	} else {
	    usage();
	}
    }

}
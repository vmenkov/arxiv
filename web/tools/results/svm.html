<html>
<head>
<title>SVM results
</title>
</head>
<body>
<h1>Applying multiclass SVM to our clustering schemes
</h1>

<center><em>Updated: 2013-10-24</em></center>

<h2>Overview</h2>

<p>This document describes several series of experiments in which we
applied
the <a href="http://www.cs.cornell.edu/people/tj/svm_light/svm_multiclass.html">Multiclass
SVM</a> tool to various clusterings generated for the recommendation
generation scheme known as EE4 (Exploration/Exploitation ver. 4).

<p>The purpose of the experiment was to understand to which extent the
paritioning of the document space into clusters produced by a
particular clustering method represents some reality that can be
"discovered" by an SVM tool. A typical experiment was carried out as
follows:
<ol>
<li>Take the clustering generated by some particular clustering method. 
<li>Optionally, select only certain clusters from it. 
<li>Select two subsets: training and test 
<li>Train the multiclass SVM on the training set; save the model
<li>Apply the model to the training set and to the test set
<li>Compare the results to the "true" cluster assignments. 
</ol>

<p>The main goal of this project was to look at how well the co-access
based clustering can be described in the term available to a SVM
learner (see <a href="#coaccess">the results here</a>); but, for
comparison, we looked also at feature-based clustering schemes, which
should be (and are) easier for SVM.

<h2>Discriminating between ArXiv categories</h2>

<p>The series of experiments described in this section has shown that
an SVM model can fairly well distinguish between ArXiv categories.

<p>To avoid having to generate a new clustering, this experiment used
our 2012 clustering scheme, which divides each of ca. 150 ArXiv
categories into a few clusters, using k-Means in the text-based
feature space. There are 492 clusters in total in this scheme. Out of
these 492 clusters, we have taken every 10th cluster, thus selecting
49 clusters; this meant that pretty much all of these 49 clusters were
in different ArXiv categories. 

<p>These 49 clusters contained in total 63108 examples, i.e. almost
10% of all documents in ArXiv. (The number is less than the full 10%,
because some older ArXiv documents are not assigned to any clusters,
due to historical changes in ArXiv's category scheme).  Form these
examples, approximately one half (31556 examples) were used as the
training set for SVM, and the other half (31552) as the test set.

<p>With this set up, the results were remarkably good: the SVM
classifier was able to correctly classify 92% of training examples, 
and nearly 90% of the test example.

<p>The not-quite-100% number on the training set probably means that
the set is not perfectly linearly separable in terms of the features
used in the SVM model. The fact that the performance on the test set
is quite close to that on the training set indicates a good quality of
the model (no overtraining).

<p>I also made runs with smaller training sets (25%, 10% etc. of the
entire set of documents in the chosen 49 categories). Not
surprisingly, one could observe that on smaller training sets, the
score of the model on the training set is better (fewer contradictory
examples, closer to being linearly separable), but the actual quality
of the model (as measured by the score on the test set) is worse.

<table border=1>
<tr><th>Size of the training set (as percentage of the entire 49 clusters)   
<th>% correct on the training set <th>% correct on the test set
<tr><th> 31556  (~50%)  <td>92.33%  <td>89.72%
<tr><th> 6312  (~10%)  <td>96.32%  <td>87.54%
<tr><th> 633  (~1%)  <td>100%  <td>76.90%
</table> 

<p>The SVM runs were done with the options -c 10 -f 2.

<h2>Discriminating between feature based clusters within the same few categories</h2>

<p>Now we take the first 49 clusters out of the same full list of 492
clusters. They are going to cover just a few major categories; so now
we are looking at SVM's ability to distinguish between minor
categories, and between feature-based clusters  within the same minor category.

<p>It was not possible to run SVM learner on the full one-half of this
49-cluster set (40510 examples); so the best results are from the one-quarter training set.

<table border=1>
<tr><th>Size of the training set (as percentage of the entire 49 clusters)   
<th>% correct on the training set <th>% correct on the test set
<tr><th> 40510  (~50%)  <td>?  <td>?
<tr><th> 20252  (~25%)  <td>75.53%  <td> 68.22%
</table> 

<p>I would say these are fairly decent results; since the correct
identification percentage even on the training set is only 75%, the
categories are apparently not fully linearly separable.

<h2><a name="492">Discriminating between (all) feature based clusters</a></h2>

<p>What I really wanted to do first was to apply the SVM learner to
all 492 feature-based clusters. However to do it with large samples,
that turned out to be not feasible due to the large size of the set,
and especially to the large number of clusters. (For a given training
set size, the Multiclass SVM tool's memory demands are naturally
higher when there are more categories). I had to limit experiments to
very small training samples.

<p>The reported runs were conducted with "-f 2 -c 1"; this option set
turned out to be more robust on this data set, memory-wise, than "-f 2
-c 10".

<table border=1>
<tr><th>Size of the training set (as percentage of the entire 49 clusters)   
<th>% correct on the training set <th>% correct on the test set
<tr><th> 1553  (~0.2%)  <td> 98.13%  <td>24.44%
<tr><th> 778  (~0.1%)  <td> 99.61% <td> 22.42%
</table> 

<p>While the results look poor, this is probably due to very small
training sizes. (1553 training examples for 492 clusters means, on
average, just over 3 examples per cluster). The results, no doubt,
would be much better with a larger training sample.

<h2><a name="coaccess">Applying SVM to the coaccess-based clustering</a></h2>

<p>This section discusses the classification task around which this
entire SVM application project was based. Peter Frazier's June 2013
plan calls for clustering documents within each major categories
(there 18 of them in ArXiv) based on the historical coaccess
matrix. (<a href="../svd.html">Here are the details.</a>)

<p>Once we have clusters created on the set of "old" documents, the
plan calls for training a multiclass SVM model on this clustering
(separately within each major category), and using this model to
assign new documents to clusters. The question arises: to which extent
these coaccess based clusters are entities that can be sensibly
described with a SVM model? To me, the "sensibility" of a model is
measured by how well a model trained on some clustered documents will
reproduce the same clustering on the rest of these documents.  The
answer is, it does so rather poorly.

<p><small>The underlying data for the following table are in 
<tt>/home/vm293/arxiv/runs/svm-halves</tt> on <tt>en-myarxiv02.orie.cornell.edu</tt>
</small>

<!--
#!/bin/csh
cd svm-halves
foreach x (*) 
if (-d $x) then
    echo '<tr><th>' $x 
    grep classes "$x/svm-$x-learn.log" |perl -pe 's/.*?, (\d+) classes/<td align=right>$1/'
    grep examples "$x/svm-$x-learn.log" |perl -pe 's/.*?(\d+).*/<td align=right>$1/'
    grep -i percentage "$x/svm-$x-classify-halves.log" | head -1  | perl -pe 's/.*percentage /<td align=right>/'
    grep -i percentage "$x/svm-$x-classify-halves.log" | tail -1  | perl -pe 's/.*percentage /<td align=right>/'
    echo '</tr>'
endif 
end
-->

<p>
<table border=1>
<tr><th>Major category
<th>Number of clusters
<th>Size of the training set
<th>% correct on the training set <th>% correct on the test set
<th>Relative size of the largest cluster
<tr><th> astro-ph
<td align=right>10
<td align=right>11779
<td align=right>74.15%
<td align=right>45.39%
<td align=right>28.39%
</tr>
<tr><th> cond-mat
<td align=right>10
<td align=right>11456
<td align=right>66.68%
<td align=right>45.90%
<td align=right>34.19%
</tr>
<tr><th> cs
<td align=right>8
<td align=right>6905
<td align=right>82.00%
<td align=right>41.69%
<td align=right>32.95%
</tr>
<tr><th> gr-qc
<td align=right>4
<td align=right>2079
<td align=right>87.21%
<td align=right>55.27%
<td align=right>40.64%
</tr>
<tr><th> hep-ex
<td align=right>3
<td align=right>903
<td align=right>99.78%
<td align=right>99.34%
<td align=right>94.35%
</tr>
<tr><th> hep-lat
<td align=right>2
<td align=right>582
<td align=right>97.42%
<td align=right>64.15%
<td align=right>60.43%
</tr>
<tr><th> hep-ph
<td align=right>6
<td align=right>4415
<td align=right>78.66%
<td align=right>51.69%
<td align=right>35.54%
</tr>
<tr><th> hep-th
<td align=right>5
<td align=right>3350
<td align=right>83.73%
<td align=right>54.81%
<td align=right>37.58%
</tr>
<tr><th> math
<td align=right>13
<td align=right>17039
<td>?
<td>?
<td align=right>30.65%
</tr>
<tr><th> math-ph
<td align=right>4
<td align=right>1676
<td align=right>93.32%
<td align=right>41.74%
<td align=right>34.83%
</tr>
<tr><th> nlin
<td align=right>2
<td align=right>750
<td align=right>98.27%
<td align=right>55.53%
<td align=right>51.17%
</tr>
<tr><th> nucl-ex
<td align=right>2
<td align=right>497
<td align=right>100.00%
<td align=right>99.80%
<td align=right>99.80%
</tr>
<tr><th> nucl-th
<td align=right>3
<td align=right>1221
<td align=right>95.41%
<td align=right>86.17%
<td align=right>65.98%
</tr>
<tr><th> physics
<td align=right>7
<td align=right>5242
<td align=right>80.01%
<td align=right>50.33%
<td align=right>39.56%
</tr>
<tr><th> q-bio
<td align=right>2
<td align=right>730
<td align=right>99.86%
<td align=right>54.66%
<td align=right>52.19%
</tr>
<tr><th> q-fin
<td align=right>2
<td align=right>451
<td align=right>99.78%
<td align=right>54.32%
<td align=right>51.22%
</tr>
<tr><th> quant-ph
<td align=right>5
<td align=right>3478
<td align=right>82.58%
<td align=right>52.14%
<td align=right>36.34%
</tr>
<tr><th> stat
<td align=right>2
<td align=right>791
<td align=right>99.87%
<td align=right>57.20%
<td align=right>50.85%
</tr>
<tr><th colspan=5>Controls
<tr><th> testA
<td align=right>4
<td align=right>1678
<td align=right>99.94%
<td align=right>99.40%
<td align=right>43.58%
</tr>
<tr><th> testB
<td align=right>3
<td align=right>1227
<td align=right>99.67%
<td align=right>97.15%
<td align=right>59.51%
</tr>

</table>

<p>
Besides the 18 real major categories ("astro-ph" through "stat"), the
table also contains two "pseudo-categories", "testA" and "testB". The
former is a union of q-stat, q-bio and nucl-ex; the latter, the union
of q-stat, q-bio. These were added as a control, to see how the
co-access clustering will behave when the data set contains two or
three subsets that are very weakly connected in the coaccess
matrix. (As expected, the clustering there "re-discovered" major
categories; similarly, it was not difficult for SVM to re-discover
this clustering).

<p>To evaluate the performance of a classifier on a particular major
category, it is useful to compare it with that of a "baseline
classifier" - a nearly-trivial classifier that simply assigns all
examples to the largest cluster. Since clusters are often of unequal
size, this baseline number may be higher than 1/(number of clusters).
For reference, the baseline number is given in the last column of the table above.

<p>For two major categories (hep-ex and nucl-ex) we see an anomalously
high percentage of correct answers. This is apparently because the
clustering on these categories produced clusters of very unequal size,
and virtually all examples were assigned to a single clusters. Thus
these two major categories should be disregarded. (One can also ask if
one should try to add a sanity check of some kind to k-Means
clustering process we're using).

<p>The overall result is that even though the SVM model is better
guessing correct clusters than the baseline, the predictive power is
not that great. Therefore my impression is that coaccess-based classes
can be only rather poorly described in terms of our feature set (terms
and minor categories).

<h4>With normalized document vectors</h4>

<p>As an experiment, we also carried out a series of SVM runs with normalized document vectors.


<p>
<table border=1>
<tr><th>Major category
<th>Number of clusters
<th>Size of the training set
<th>% correct on the training set <th>% correct on the test set
<th>Relative size of the largest cluster
<tr><th> astro-ph
<td align=right>10
<td align=right>11779
<td align=right>23.34%
<td align=right>21.76%
<td align=right>28.39%
</tr>
<tr><th> cond-mat
<td align=right>10
<td align=right>11456
<td align=right>26.22%
<td align=right>25.29%
<td align=right>34.19%
</tr>
<tr><th> cs
<td align=right>8
<td align=right>6905
<td align=right>13.14%
<td align=right>11.40%
<td align=right>32.95%
</tr>
<tr><th> gr-qc
<td align=right>4
<td align=right>2079
<td align=right>41.17%
<td align=right>39.78%
<td align=right>40.64%
</tr>
<tr><th> hep-ex
<td align=right>3
<td align=right>903
<td align=right>93.58%
<td align=right>95.13%
<td align=right>94.35%
</tr>
<tr><th> hep-lat
<td align=right>2
<td align=right>582
<td align=right>60.82%
<td align=right>60.21%
<td align=right>60.43%
</tr>
<tr><th> hep-ph
<td align=right>6
<td align=right>4415
<td align=right>40.54%
<td align=right>39.39%
<td align=right>35.54%
</tr>
<tr><th> hep-th
<td align=right>5
<td align=right>3350
<td align=right>37.64%
<td align=right>35.64%
<td align=right>37.58%
</tr>
<tr><th> math
<td align=right>13
<td align=right>17039
<td align=right>12.64%
<td align=right>12.15%
<td align=right>30.65%
</tr>
<tr><th> math-ph
<td align=right>4
<td align=right>1676
<td align=right>48.15%
<td align=right>32.32%
<td align=right>34.83%
</tr>
<tr><th> nlin
<td align=right>2
<td align=right>750
<td align=right>79.07%
<td align=right>53.53%
<td align=right>51.17%
</tr>
<tr><th> nucl-ex
<td align=right>2
<td align=right>497
<td align=right>99.80%
<td align=right>99.80%
<td align=right>99.80%
</tr>
<tr><th> nucl-th
<td align=right>3
<td align=right>1221
<td align=right>67.57%
<td align=right>68.09%
<td align=right>65.98%
</tr>
<tr><th> physics
<td align=right>7
<td align=right>5242
<td align=right>47.27%
<td align=right>44.54%
<td align=right>39.56%
</tr>
<tr><th> q-bio
<td align=right>2
<td align=right>730
<td align=right>68.49%
<td align=right>54.79%
<td align=right>52.19%
</tr>
<tr><th> q-fin
<td align=right>2
<td align=right>451
<td align=right>72.95%
<td align=right>53.88%
<td align=right>51.22%
</tr>
<tr><th> quant-ph
<td align=right>5
<td align=right>3478
<td align=right>42.64%
<td align=right>39.24%
<td align=right>36.34%
</tr>
<tr><th> stat
<td align=right>2
<td align=right>791
<td align=right>73.83%
<td align=right>55.81%
<td align=right>50.85%
</tr>
<tr><th> testA
<td align=right>4
<td align=right>1678
<td align=right>98.33%
<td align=right>98.39%
<td align=right>43.58%
</tr>
<tr><th> testB
<td align=right>3
<td align=right>1227
<td align=right>95.60%
<td align=right>94.79%
<td align=right>59.51%
</tr>
</table>

</body>
</html>


<html>
<head>
<title>SVD clustering
</title>
</head>
<body>
<h1>SVD clustering: How-to guide
</h1>

<h2>Peter Frazer's original specs
</h2>

<center><em>2013-10-13</em></center>

<p>
(From 2013-06-07 message)

<pre>
We split up papers into what we are calling "super-categories", which are
disjoint unions of arxiv categories.  I am envisioning that we would have one
super-category for each of the bulletted list of categories at the main page on
arxiv.org.  So for example, one super-category would correspond to all the
astro-ph categories, one to all the cond-mat categories, etc.  Currently we are
doing our analysis on the hep-th super-category.

1. For each super-category, do the following:

a. Create a ratings matrix from historical data, with users on the rows, and papers on the columns.  We do this as follows:
    Let Users be the set of users who viewed at least user_thresh=2 papers in the super-category in the given time-period (2010-2012).
    Let Items be the set of papers meeting the following criteria:
        primary category is in the super-category
        submitted in the given time-period (2010-2011)
        viewed by at least paper_thresh=2 users from Users
    Create a matrix R with rows in Users and columns in Items, with a 1 at cell (u,i) if user u viewed paper i, and a 0 otherwise.
    user_thresh and paper_thresh are both tunable parameters, which we might play around with at some point, as is the time period.

1b. Do an SVD of the ratings matrix, as per the standard approach in low-rank
matrix factorization for recommender systems.  So in particular, compute
matrices U, D, and V where R = U D V^T, U and V are orthonormal matrices, D is
diagonal and has entries sorted from largest to smallest, where U has |Users|
rows, and V has |Items| rows.  Then drop all but the top k_svd entries in D, and
drop the corresponding rows in U and V, so that U and V now both have k_svd
columns, and we are approximating R by  U D V^T.  k_svd is a tunable parameter,
but we are currently feeling good about k_svd=5.

1c. Use kmeans to cluster the rows of V (which now has |Items| rows and k_svd
columns), as points in kvsd-dimensional space.  Use k_kmeans clusters.  Again,
k_kmeans is a tunable parameter, and we want to do some back-testing to figure
out what will work well, but it probably makes sense to set it the same way we
were setting it for the original clustering scheme you implemented.

1d. Now, for each paper in Items, we have a kmeans cluster label.  Using this
as training data, train an SVM multiclass classifier to predict the kmeans
cluster label from your custom features, which include TF-IDF and authors,
etc., and also adding to this set of features the primary category of the
paper, and the binary vector listing which secondary categories are turned on.


2.  When a new paper arrives, figure out what its super category is, and then
take the corresponding trained multiclass SVM, and apply it to the paper's
feature vector to predict its cluster label.  So for example, if a paper is in
the astro-ph super category, we will use the astro-ph multi-class SVM.  Say
that SVM predicts a cluster label of 6.  Then the paper's cluster will be
"astro-ph:6".

</pre>

<h2>Overall arrangements</h2>

<p>These instructions are for en-myarxiv02.orie.cornell.edu

<p>The main data directory is  /data/arxiv/arXiv-data. Make sure to set up a symbolic link so that you can access it via ~/arxiv/arXiv-data.  (I.e.:
<pre>
   cd ~
   mkdir arxiv
   cd ~/arxiv
   ln -s  /data/arxiv/arXiv-data .
</pre>

<p>The scripts are designed for running in <tt>
$home/arxiv/arxiv </tt>. The shell variable <tt>$home</tt> (in C
shell, at any rate) refers to your home directory; it's the same thing
as "~". In my case, $home=/home/vm293 ; for you, it will be your home
directory. For everything to work smoothly, you may need to sym-link a
few important directories into your ~/arxiv, e.g.
<pre>
   cd ~/arxiv
   ln -s  ~vm293/arxiv/lib .
   ln -s  ~vm293/arxiv/lucene .
   <em>Maybe a few more similar sym links... you'll see if you need them
    based on "No class found" errors etc.</em>
   mkdir arxiv
</pre>
This is necessary in order for your Java application to find
certain <tt>jar</tt> files. (You can check the list of jar files
needed by each script by looking inside each script, paying attention
to how the classpath is constructed.)

<p>A number of shell scripts and perl scripts will be discussed
below. They reside in ~vm293/arxiv/arxiv. Before running them, you can
copy them to ~/arxiv/arxiv (i.e., the analogously named directory in
your own directory tree). Alternatively, you can check everything out from 
 <a href="https://forge.cornell.edu/sf/projects/my_arxiv_org">the Cornell SourceFourge repository for the Project my.arxiv.org</a>

<p>Below, unless explicitly indicated otherwise, it will be assumed that all commands are run in the directory <tt>~/arxiv/arxiv</tt> .

<h2>Splitting usage data</h2>

<p>The 10+ years of the usage data files, uploaded by Dr. Ginsparg, are in 
<tt> /data/json/usage </tt>. The first step of processing them for our purposes is splitting them into 18 major categories ("super categories", in Peter's terminology). Since this has been done already (for years 2009-2012), you probably will <strong>not</strong> need to re-do it. Nonetheless, for completeness, here's the command:

<pre>
  ./json.sh
</pre>

If you look inside the script, you'll see that, after setting the classpath etc, it does the following:
<pre>
foreach f (/data/json/usage/201[012]/*.json.gz) 
    date
    echo Splitting file $f
    time java $opt  edu.rutgers.axs.ee4.HistoryClustering split $f
end
</pre>

<p>In a separate run, the usage files for 2009 have been split as well.

<p>All split files are written into  /data/arxiv/arXiv-data/tmp/hc

<h2>Running SVD and clustering</h2>

<p>The next script carries out, separately for each major category, the following steps:
<ul>
<li>Creation of the coaccess matrix from the usage data. At this step,
split files (generated at the previous step) are read; "inappropriate"
articles (those with a creation date outside of the specified date
range) are excluded, and the list of users and articles may be pruned
as well, based on Peter's criteria. Of course we don't actually know
"who is who", user-wise; what the program does is going by the
cookies: all activities associated with the same cookie are treated as
carries out by the same user; and if multiple cookies are linked to
the same registered user (as per /data/json/usage/tc.json.gz ), then
they all are interpreted as the same user, too.

<li>
Incomplete SVD factoring of the co-access matrix. The parameter "k_svd" is customizable; this is how many top singular values will be found.

<li>
The k-Means clustering of the article set in the reduced-dimension (k_svd-dimensional) space. The number of clusters is controlled by the parameter "k_kmeans". if it is not set, or is set to 0, then the number of clusters will be determined adaptively, by the formula
<pre>
	    k_kmeans = (int)Math.sqrt(  N / 200.0),
</pre>
where N is the number of articles in the super-category (that is,
the number of articles that remain after all ineligible articles have been removed).
</ul>

<p>
The assignment map will be written, by default, to the file <tt>
/data/arxiv/arXiv-data/tmp/svd-asg/$cat/asg.dat </tt>,
where $cat is the major category name ("math", "astro-ph", etc).

<p>The command to run this step is
<pre>
./json-svd.sh
</pre>

<p>This is what inside the script:

<pre>
#-- List of categories to process. This command simply lists all subdirectories
#-- in ../arXiv-data/tmp/hc , so that all major cats are processed. For
#-- testing purposes, you can have a shorter list instead
set cats=`(cd ../arXiv-data/tmp/hc; /bin/ls)`
date


#-- Specify various options:
# -DusageFrom=20100101 -DusageTo=20130101   : the range of usage files
# -DarticleDateFrom=20100101 -DarticleDateTo=20120101 : article submission dates
# -Dk_svd=5 : the number of singular vectors to keep
# -Dk_kmeans=0 : the number of clusters to create. (0 means setting the number adaptively, based on the set size)
set opt="$baseopt -DusageFrom=20100101 -DusageTo=20130101 -DarticleDateFrom=20100101 -DarticleDateTo=20120101 -Dk_svd=5 -Dk_kmeans=0"


foreach cat ($cats) 
 echo Processing category $cat

 # One can add an option such as 
 # -DasgPath=../arXiv-data/tmp/svd-asg/$cat/asg.dat right after $opt
 # to control the location of the output file (the assignment map)

 time java $opt  edu.rutgers.axs.ee4.HistoryClustering svd $cat >& svd-${cat}.log
end

</pre>

<p>The output file (the assignment map) for each category $cat will go
to the file <tt> ../arXiv-data/tmp/svd-asg/$cat/asg.dat </tt>, unless indicated otherwise using the option -DasgPath=... 

<p><strong>You probably want to set the  option -DasgPath=... to something reasonable (e.g.  -DasgPath=./my-svd-out-dir ), so that the files will go to your own, writeable, data directory.
</strong>

<p>To run SVD with different options, you can simply copy this script to a script of your own and modify the options as required.

<h2>Assessing clustering quality</h2>

<p>There are a few tools for checking out on the quality of your clustering.

<h3>File locations</h3>

<p>
The assignment maps for all super categories are in
<tt>/data/arxiv/arXiv-data/tmp/svd-asg</tt>
The log files from my clustering tool run are in
<tt>/home/vm293/arxiv/arxiv/logs/svd</tt>

<p>In your own clustering runs, you can conrol the output files by modifying the shell script (jason-svd.sh) as desired.

<h3>Checking singular values</h3>

<pre>
% cd /home/vm293/arxiv/arxiv/logs/svd
% grep values svd-math-5.log
Top 5 singular values: 421.5746912265288 392.1722034100277
255.29508914447823 241.239429802076 235.5894627912343
</pre>

<p>
So this shows that we had 26 clusters for the category "math". (The
number of clusters is chosen based on the category size). For each
super-cat we do 10 runs, and choose the best; the above report shows
that out of the 10 clustering runs, the clustering scheme from run no.
8 was chosen...

<pre>
% grep 'scheme no. 8' svd-math-5.log
Clustering scheme no. 8: {pop=8479 |c|=0.003684813868064367
|r|=7.568944337459525E-4}, {pop=6138 |c|=0.004479121211981062
|r|=0.001301160030551117}, {pop=191 |c|=0.026135334603835353
|r|=0.0076945166942782185}, {pop=4383 |c|=0.005446239633873648
|r|=0.001414896394050918}, ....
</pre>

<p>
The above shows stats for all 26 clusters in the chosen clustering
scheme. The stats for each cluster are in the following format:
<pre>
{pop= <em>cluster size</em>
 |c|= <em>the Euclidean norm of the cluster's centroid (i.e., the distance to origin)</em>
  |r|=  <em>sqrt( <|x-center|^2>,  the "radius" of the cluster, i.e. the
square root of the average distance of cluster members to the cluster's centroid</em>
}

<h3>How good is the clustering?</h3>

<p> The following tool can be used to
review the list of articles assigned to a given cluster:

<pre>
% cd /home/vm293/arxiv/arxiv
% ./showCluster.sh /data/arxiv/arXiv-data/tmp/svd-asg/physics/asg.dat 1 | more
[1105.0968] physics.ao-ph; Paleoclimate Implications for Human-Made
Climate Change
[1104.2026] physics.soc-ph cs.GT cs.SI; Collaboration in Social Networks
[1104.1789] physics.soc-ph cs.SI; Zipf's law unzipped
[1104.1788] physics.pop-ph physics.soc-ph; Trees as Filters of
Radioactive Fallout from the Chernobyl Accident
...
</pre>

<p>
This displays the article id, the categories, and the title for each
article in cluster no. 1 of the "physics" super-cat.

<p>
One of the reasons I am broadcasting this note is because I am not all
that sure that the quality of the clustering we generate is all that
good; so I am inviting those of us who have interests in particular
subject areas to take a look at how their categories of interests are
clustered.


</body>
</html>
